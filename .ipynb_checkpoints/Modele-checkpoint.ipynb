{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from unidecode import unidecode\n",
    "import string\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from french_lefff_lemmatizer.french_lefff_lemmatizer import FrenchLefffLemmatizer\n",
    "import spacy\n",
    "import numpy as np\n",
    "import fr_core_news_sm\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\"> Préparation et nettoyage de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corpus de traitement de données\n",
    "corpus= open(\"./CorpusM2-AFD/corpus.tache1.learn\",\"r\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"Quand je dis chers amis, il ne s'agit pas là d'une formule diplomatique, mais de l'expression de ce que je ressens.\", \"D'abord merci de cet exceptionnel accueil que les Congolais, les Brazavillois, nous ont réservé cet après-midi.\", \"C'est toujours très émouvant de venir en Afrique car c'est probablement l'une des rares terres du monde où l'on ait conservé cette convivialité, cette amitié, ce respect de l'autre qui s'expriment avec chaleur, avec spontanéité et qui réchauffent le coeur de ceux qui arrivent et de ceux qui reçoivent.\", 'Aucun citoyen français ne peut être indifférent à un séjour à Brazzaville.', 'Le Congo, que naguère le <nom> qualifia de \"refuge pour la liberté\", de \"base de départ pour la libération\", de \"môle pour la Résistance\", comment ne pas être heureux de s\\'y retrouver ?']\n",
      "['C', 'C', 'C', 'C', 'C']\n"
     ]
    }
   ],
   "source": [
    "# Methode permettant de retourner la cibre soit C ou M\n",
    "def get_Target(s):\n",
    "    for i in range(0,len(s)):\n",
    "        if s[i]== '>':\n",
    "            return s[i-1]\n",
    "\n",
    "# Methode permettant de retourner les données X(entrée) et Y(cible) et le corpus sous forme d'une liste \n",
    "def get_X_Y(corpus):\n",
    "    X=[]\n",
    "    Y=[]\n",
    "    Data=[]\n",
    "    Target=[]\n",
    "    for ligne in corpus:\n",
    "        data=ligne.split()\n",
    "        Target.append(data[0])\n",
    "        Data.append(ligne)\n",
    "        X.append(\" \".join(data[1:]))\n",
    "        Y.append(get_Target(data[0]))\n",
    "    return X,Y,Data,Target\n",
    "\n",
    "X,Y,data,target=get_X_Y(corpus)\n",
    "print(X[0:5])\n",
    "print(Y[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_MajusculesToMinuscules: \n",
      "[\"quand je dis chers amis, il ne s'agit pas là d'une formule diplomatique, mais de l'expression de ce que je ressens.\", \"d'abord merci de cet exceptionnel accueil que les congolais, les brazavillois, nous ont réservé cet après-midi.\", \"c'est toujours très émouvant de venir en afrique car c'est probablement l'une des rares terres du monde où l'on ait conservé cette convivialité, cette amitié, ce respect de l'autre qui s'expriment avec chaleur, avec spontanéité et qui réchauffent le coeur de ceux qui arrivent et de ceux qui reçoivent.\", 'aucun citoyen français ne peut être indifférent à un séjour à brazzaville.', 'le congo, que naguère le <nom> qualifia de \"refuge pour la liberté\", de \"base de départ pour la libération\", de \"môle pour la résistance\", comment ne pas être heureux de s\\'y retrouver ?']\n"
     ]
    }
   ],
   "source": [
    "# Methode permettant de transformer une lettre en majuscule à une lettre en miniscule  \n",
    "def Transformer_MajusculesToMinuscules(l):\n",
    "    X=[]\n",
    "    for element in l:\n",
    "            X.append(element.lower())\n",
    "    return X\n",
    "print(\"Transformer_MajusculesToMinuscules: \")\n",
    "X = Transformer_MajusculesToMinuscules(X)\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lemmatisation: \n",
      "['quand je dis cher ami , il ne se agir pas là de un formule diplomatique , mais de le expression de ce que je ressentir .', 'de abord merci de ce exceptionnel accueil que le congolais , le brazavilloi , nous avoir réserver ce après-midi .', 'ce être toujours très émouvoir de venir en afrique car ce être probablement le un un rare terre de monde où le on avoir conserver ce convivialité , ce amitié , ce respect de le autre qui se exprimer avec chaleur , avec spontanéité et qui réchauffer le coeur de celui qui arriver et de celui qui recevoir .', 'aucun citoyen français ne pouvoir être indifférer à un séjour à brazzaville .', 'le congo , que naguère le < nom > qualifia de \" refuge pour le liberté \" , de \" base de départ pour le libération \" , de \" môle pour le résistance \" , comment ne pas être heureux de se y retrouver ?']\n"
     ]
    }
   ],
   "source": [
    "# Methode permettant de lemmatiser les mots (verbre conjugué devient infinitif et pluriel devient singulier )\n",
    "def Lemmatisation(X):\n",
    "    nlp = spacy.load(\"fr\")\n",
    "    NewList=[]\n",
    "    for sentence in X:\n",
    "        new_sentence=[]\n",
    "        sentence = nlp(sentence)\n",
    "        new_sentence =[word.lemma_ for word in sentence]\n",
    "        NewList.append(' '.join([str(elem) for elem in new_sentence]))\n",
    "    return NewList\n",
    "\n",
    "print(\"Lemmatisation: \")\n",
    "X = Lemmatisation(X)\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stop_Words: \n",
      "['quand dis cher ami , agir là formule diplomatique , expression ressentir .', 'abord merci exceptionnel accueil congolais , brazavilloi , avoir réserver après-midi .', 'être toujours très émouvoir venir afrique car être probablement rare terre monde où avoir conserver convivialité , amitié , respect autre exprimer chaleur , spontanéité réchauffer coeur celui arriver celui recevoir .', 'aucun citoyen français pouvoir être indifférer séjour brazzaville .', 'congo , naguère < nom > qualifia \" refuge liberté \" , \" base départ libération \" , \" môle résistance \" , comment être heureux retrouver ?']\n"
     ]
    }
   ],
   "source": [
    "# Methode permettant d'enlever les stop words (de , sur, le, nous, je, déjà, plusieurs) \n",
    "def Stop_Words(X):\n",
    "    stop_words = set(stopwords.words('french'))\n",
    "    listToStr=[]\n",
    "    for sentence in X:\n",
    "        wordsFiltered=[]\n",
    "        sentence=sentence.split()\n",
    "        for word in sentence:\n",
    "            if word not in stop_words:\n",
    "                wordsFiltered.append(word)\n",
    "        listToStr.append(' '.join([str(elem) for elem in wordsFiltered]))\n",
    "    return listToStr\n",
    "\n",
    "print(\"Stop_Words: \")\n",
    "X = Stop_Words(X)\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Remove_Punctuation: \n",
      "['quand dis cher ami  agir là formule diplomatique  expression ressentir ', 'abord merci exceptionnel accueil congolais  brazavilloi  avoir réserver aprèsmidi ', 'être toujours très émouvoir venir afrique car être probablement rare terre monde où avoir conserver convivialité  amitié  respect autre exprimer chaleur  spontanéité réchauffer coeur celui arriver celui recevoir ', 'aucun citoyen français pouvoir être indifférer séjour brazzaville ', 'congo  naguère  nom  qualifia  refuge liberté    base départ libération    môle résistance   comment être heureux retrouver ']\n"
     ]
    }
   ],
   "source": [
    "# Methode permettant d'enlever les ponctuations\n",
    "def Remove_Punctuation(X):\n",
    "    list=[]\n",
    "    for sentence in X:\n",
    "        stripped=[]\n",
    "        table = str.maketrans('', '', string.punctuation)\n",
    "        sentence=sentence.split()\n",
    "        stripped = [word.translate(table) for word in sentence]\n",
    "        list.append(' '.join([str(elem) for elem in stripped]))\n",
    "    return list\n",
    "\n",
    "print(\"Remove_Punctuation: \")\n",
    "X = Remove_Punctuation(X)\n",
    "print(X[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tf_IDF: \n",
      "(57413, 143984)\n",
      "4.102251963804088\n",
      "3.4387924624451607\n",
      "3.997048371667066\n",
      "1.920866138884879\n"
     ]
    }
   ],
   "source": [
    "# Methode retourne le TFIDF de chaque phrase ou bien donnée d'entrée\n",
    "def Tf_IDF(X):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(1,5),min_df=2)\n",
    "    X_tfidf = vectorizer.fit_transform(X)\n",
    "    return X_tfidf,vectorizer\n",
    "\n",
    "print(\"Tf_IDF: \")\n",
    "X_tfifd,vect = Tf_IDF(X)\n",
    "print(X_tfifd.shape)\n",
    "print(np.sum(X_tfifd[0].toarray()))\n",
    "print(np.sum(X_tfifd[1].toarray()))\n",
    "\n",
    "print(np.sum(X_tfifd[15].toarray()))\n",
    "print(np.sum(X_tfifd[16].toarray()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, -1, 1, 1, 1, 1, 1]\n"
     ]
    }
   ],
   "source": [
    "# Methode permettant de changer la lettre de target sous forme d'entier soit 1 pour M et -1 pour C\n",
    "def Convertir_Target_ToBis(Y):\n",
    "    New_list_Target=[]\n",
    "    for y in Y:\n",
    "        if y=='M':\n",
    "            New_list_Target.append(1)\n",
    "        else:\n",
    "            New_list_Target.append(-1)\n",
    "\n",
    "    return New_list_Target\n",
    "\n",
    "# Methode permettant de changer l'entier de target sous forme de lettre soit M pour 1 et C pour -1\n",
    "def Convertir_Target_ToLetter(Y):\n",
    "    New_list_Target=[]\n",
    "    for y in Y:\n",
    "        if y==1:\n",
    "            New_list_Target.append('M')\n",
    "        else:\n",
    "            New_list_Target.append('C')\n",
    "\n",
    "    return New_list_Target\n",
    "\n",
    "Y = Convertir_Target_ToBis(Y)\n",
    "print(Y[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(45930, 143984)\n",
      "(11483, 143984)\n"
     ]
    }
   ],
   "source": [
    "# il permet de separer le corpus en deux corpus (corpus d'entrainement et corpus de Test) et recuperer leur valeurs d'entrées et de sorties\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_tfifd, Y, test_size=0.2,shuffle=False)\n",
    "\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocutions: \n",
      "[[0, 12], [12, 62], [62, 173], [173, 299], [299, 380], [380, 483], [483, 642], [642, 691], [691, 789], [789, 807], [807, 1107], [1107, 1162], [1162, 1262], [1262, 1364], [1364, 1409], [1409, 1509], [1509, 1556], [1556, 1718], [1718, 1791], [1791, 1968], [1968, 2245], [2245, 2306], [2306, 2318], [2318, 2443], [2443, 2466], [2466, 2586], [2586, 2694], [2694, 2724], [2724, 2830], [2830, 2873], [2873, 2932], [2932, 3098], [3098, 3231], [3231, 3295], [3295, 3336], [3336, 3378], [3378, 3397], [3397, 3551], [3551, 3652], [3652, 3849], [3849, 3881], [3881, 3938], [3938, 3976], [3976, 4156], [4156, 4352], [4352, 4572], [4572, 4636], [4636, 4683], [4683, 4815], [4815, 4944], [4944, 5047], [5047, 5092], [5092, 5279], [5279, 5344], [5344, 5421], [5421, 5585], [5585, 5615], [5615, 5671], [5671, 5894], [5894, 5969], [5969, 6005], [6005, 6146], [6146, 6390], [6390, 6504], [6504, 6655], [6655, 6704], [6704, 6815], [6815, 6841], [6841, 6969], [6969, 7111], [7111, 7149], [7149, 7180], [7180, 7270], [7270, 7325], [7325, 7437], [7437, 7518], [7518, 7551], [7551, 7606], [7606, 7653], [7653, 7714], [7714, 7739], [7739, 7857], [7857, 7938], [7938, 8033], [8033, 8110], [8110, 8142], [8142, 8246], [8246, 8293], [8293, 8365], [8365, 8403], [8403, 8454], [8454, 8527], [8527, 8633], [8633, 8684], [8684, 8819], [8819, 8932], [8932, 9012], [9012, 9146], [9146, 9292], [9292, 9369], [9369, 9451], [9451, 9600], [9600, 9661], [9661, 9702], [9702, 9763], [9763, 9825], [9825, 9884], [9884, 10003], [10003, 10089], [10089, 10151], [10151, 10205], [10205, 10247], [10247, 10440], [10440, 10525], [10525, 10560], [10560, 10730], [10730, 10900], [10900, 11005], [11005, 11090], [11090, 11483]]\n"
     ]
    }
   ],
   "source": [
    "# Methode permettant de retourner les indices de debut et de fin de chaque allocution \n",
    "def discours(data,deb,X_tfifd):\n",
    "    res = []\n",
    "    fin = X_tfifd.shape[0]\n",
    "    ideb = 0\n",
    "    ifin = -1\n",
    "    new = 0\n",
    "    for i in range(deb,fin):\n",
    "        doc1 = data[i].split(' ')\n",
    "        doc2 = doc1[0].split(':')\n",
    "        if(doc2[0][1:] != new):\n",
    "            ifin += 1\n",
    "            res.append([ideb,ifin])\n",
    "            new = doc2[0][1:]\n",
    "            ideb = ifin\n",
    "        else:\n",
    "            ifin += 1\n",
    "    res.append([ideb,ifin + 1])\n",
    "    return res[1:]\n",
    "allocutions = discours(data,X_train.shape[0],X_tfifd)\n",
    "print(\"Allocutions: \")\n",
    "print(allocutions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode permettant de faire le premier post traitement \n",
    "def post_traitement_1(tab):\n",
    "    res = [tab[0]]\n",
    "    for i in range(1,len(tab)-1):\n",
    "        if tab[i+1] == 1 or tab[i-1] == 1:\n",
    "            res.append(1)\n",
    "        else:\n",
    "            res.append(tab[i])\n",
    "    res.append(tab[len(tab)-1])\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python program to print largest contiguous array sum \n",
    "from sys import maxsize \n",
    "\n",
    "# Methode permettant de trouver le maximum contiguous subarray et afficher ses indices de depart et de fin\n",
    "def maxSubArraySum(a,size): \n",
    "\n",
    "    max_so_far = -maxsize - 1\n",
    "    max_ending_here = 0\n",
    "    start = 0\n",
    "    end = 0\n",
    "    s = 0\n",
    "\n",
    "    for i in range(0,size): \n",
    "\n",
    "        max_ending_here += a[i] \n",
    "\n",
    "        if max_so_far < max_ending_here: \n",
    "            max_so_far = max_ending_here \n",
    "            start = s \n",
    "            end = i \n",
    "\n",
    "        if max_ending_here < 0: \n",
    "            max_ending_here = 0\n",
    "            s = i+1\n",
    "    return start,end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode permettant d'appliquer le maximum contiguous subarray sur une seul allocution\n",
    "def post_traitement_2_aux(tab):\n",
    "    deb,fin = maxSubArraySum(tab,len(tab))\n",
    "    if(deb == 0 and fin == 0):\n",
    "        return np.asarray(tab)\n",
    "    else:\n",
    "        res = []\n",
    "        for i in range(0,deb):\n",
    "            res.append(-1)\n",
    "        for i in range(deb,fin+1):\n",
    "            res.append(1)\n",
    "        for i in range(fin+1,len(tab)):\n",
    "            res.append(-1)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Methode permettant d'appliquer le maximum contiguous subarray sur toutes les allocutions\n",
    "def post_traitement_2(tab):\n",
    "    res_tot = []\n",
    "    i = 0\n",
    "    for t in allocutions:      \n",
    "        res_t = post_traitement_2_aux(post_traitement_1(tab[t[0]:t[1]]))\n",
    "        res_tot = np.concatenate([res_tot, res_t]).astype(int)\n",
    "    return res_tot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\"> Classifieur 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.96      0.98      0.97     10320\n",
      "           1       0.77      0.63      0.70      1163\n",
      "\n",
      "    accuracy                           0.94     11483\n",
      "   macro avg       0.87      0.81      0.83     11483\n",
      "weighted avg       0.94      0.94      0.94     11483\n",
      "\n",
      "0.9440041800923104\n"
     ]
    }
   ],
   "source": [
    "# Premier classifieur sur la regression logistique\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "\n",
    "clf_LOG = LogisticRegression(solver='lbfgs',multi_class='multinomial')\n",
    "clf_LOG.fit(X_train, y_train)\n",
    "\n",
    "y_pred_1 = clf_LOG.predict(X_test)\n",
    "y_pred_1 =post_traitement_2(y_pred_1)\n",
    "\n",
    "clf_LOG.score(X_test, y_test)\n",
    "print(classification_report(y_test,y_pred_1))\n",
    "print(accuracy_score(y_test, y_pred_1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\">  Classifieur 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.97      0.97      0.97     10320\n",
      "           1       0.72      0.77      0.75      1163\n",
      "\n",
      "    accuracy                           0.95     11483\n",
      "   macro avg       0.85      0.87      0.86     11483\n",
      "weighted avg       0.95      0.95      0.95     11483\n",
      "\n",
      "0.9471392493250893\n"
     ]
    }
   ],
   "source": [
    "# Deuxième classifieur sur le SVM (Séparateur à vaste marge)\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "clf_SVC = LinearSVC()\n",
    "clf_SVC.fit(X_train, y_train)\n",
    "\n",
    "y_pred_2 = clf_SVC.predict(X_test)\n",
    "y_pred_2 =post_traitement_2(y_pred_2)\n",
    "\n",
    "clf_SVC.score(X_test, y_test)\n",
    "print(classification_report(y_test,y_pred_2))\n",
    "print(accuracy_score(y_test, y_pred_2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\">  Classifieur 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "          -1       0.98      0.93      0.96     10320\n",
      "           1       0.58      0.86      0.69      1163\n",
      "\n",
      "    accuracy                           0.92     11483\n",
      "   macro avg       0.78      0.89      0.82     11483\n",
      "weighted avg       0.94      0.92      0.93     11483\n",
      "\n",
      "0.9224070364887225\n"
     ]
    }
   ],
   "source": [
    "# Troisième classifieur sur Linear Discriminant Analysis\n",
    "from sklearn.linear_model import Perceptron\n",
    "\n",
    "clf_P = Perceptron(tol=1e-3, random_state=0)\n",
    "clf_P.fit(X_train, y_train)\n",
    "\n",
    "y_pred_3 = clf_P.predict(X_test)\n",
    "y_pred_3 = post_traitement_2(y_pred_3)\n",
    "\n",
    "clf_P.score(X_test, y_test)\n",
    "print(classification_report(y_test,y_pred_3))\n",
    "print(accuracy_score(y_test, y_pred_3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\"> Prédictions sur les données du fichier tache1.test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Corpus de traitement de données\n",
    "####################################\n",
    "corpus_tache1_test= open(\"./CorpusM2-AFD/corpus.tache1.test\",\"r\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_MajusculesToMinuscules: \n",
      "Lemmatisation: \n",
      "Stop_Words: \n",
      "Remove_Punctuation: \n",
      "Tf_IDF: \n"
     ]
    }
   ],
   "source": [
    "##############################\n",
    "# Recuperation des données\n",
    "##############################\n",
    "X_Test_1,Y,data,Target_Test_1=get_X_Y(corpus_tache1_test)\n",
    "\n",
    "############################\n",
    "# Traitement des données\n",
    "############################\n",
    "print(\"Transformer_MajusculesToMinuscules: \")\n",
    "X_Test_1 = Transformer_MajusculesToMinuscules(X_Test_1)\n",
    "\n",
    "print(\"Lemmatisation: \")\n",
    "X_Test_1 = Lemmatisation(X_Test_1)\n",
    "\n",
    "print(\"Stop_Words: \")\n",
    "X_Test_1 = Stop_Words(X_Test_1)\n",
    "\n",
    "print(\"Remove_Punctuation: \")\n",
    "X_Test_1 = Remove_Punctuation(X_Test_1)\n",
    "\n",
    "print(\"Tf_IDF: \")\n",
    "X_Test_1 = vect.transform(X_Test_1)\n",
    "\n",
    "allocutions = discours(data,0,X_Test_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################################\n",
    "# Prédictions du premier classifieur\n",
    "#######################################\n",
    "y_pred_1 = clf_LOG.predict(X_Test_1)\n",
    "y_pred_1 =post_traitement_2(y_pred_1)\n",
    "y_pred_1=Convertir_Target_ToLetter(y_pred_1)\n",
    "Classifieur_1_tache1_test = open(\"corpus.tache1.pred1\",\"w\") \n",
    "\n",
    "for (t,lettre) in zip(Target_Test_1,y_pred_1):\n",
    "    t=t.strip(\">\")\n",
    "    Classifieur_1_tache1_test.write(t+\":\"+lettre+\">\\n\") \n",
    "\n",
    "Classifieur_1_tache1_test.close() \n",
    "\n",
    "########################################\n",
    "# Prédictions du deuxième classifieur\n",
    "########################################\n",
    "y_pred_2 = clf_SVC.predict(X_Test_1)\n",
    "y_pred_2 =post_traitement_2(y_pred_2)\n",
    "y_pred_2=Convertir_Target_ToLetter(y_pred_2)\n",
    "\n",
    "Classifieur_2_tache1_test = open(\"corpus.tache1.pred2\",\"w\") \n",
    "\n",
    "for (t,lettre) in zip(Target_Test_1,y_pred_2):\n",
    "    t=t.strip(\">\")\n",
    "    Classifieur_2_tache1_test.write(t+\":\"+lettre+\">\\n\") \n",
    "\n",
    "Classifieur_2_tache1_test.close() \n",
    "\n",
    "#########################################\n",
    "# Prédictions du troisième classifieur\n",
    "#########################################\n",
    "y_pred_3 = clf_P.predict(X_Test_1)\n",
    "y_pred_3 =post_traitement_2(y_pred_3)\n",
    "y_pred_3=Convertir_Target_ToLetter(y_pred_3)\n",
    "\n",
    "Classifieur_3_tache1_test = open(\"corpus.tache1.pred3\",\"w\") \n",
    "\n",
    "for (t,lettre) in zip(Target_Test_1,y_pred_3):\n",
    "    t=t.strip(\">\")\n",
    "    Classifieur_3_tache1_test.write(t+\":\"+lettre+\">\\n\") \n",
    "\n",
    "Classifieur_3_tache1_test.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"color:green; font-family:Georgia; font-size:2.1m;\"> Prédictions sur les données du fichier tache2.test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################################\n",
    "# Corpus de traitement de données\n",
    "####################################\n",
    "corpus_tache2_test= open(\"./CorpusM2-AFD/corpus.tache2.test\",\"r\",encoding=\"latin-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer_MajusculesToMinuscules: \n",
      "Lemmatisation: \n",
      "Stop_Words: \n",
      "Remove_Punctuation: \n",
      "Tf_IDF: \n"
     ]
    }
   ],
   "source": [
    "#############################\n",
    "# Recuperation des données\n",
    "#############################\n",
    "X_Test_2,Y,data,Target_Test_2=get_X_Y(corpus_tache2_test)\n",
    "\n",
    "############################\n",
    "# Traitement des données\n",
    "############################\n",
    "print(\"Transformer_MajusculesToMinuscules: \")\n",
    "X_Test_2 = Transformer_MajusculesToMinuscules(X_Test_2)\n",
    "\n",
    "print(\"Lemmatisation: \")\n",
    "X_Test_2 = Lemmatisation(X_Test_2)\n",
    "\n",
    "print(\"Stop_Words: \")\n",
    "X_Test_2 = Stop_Words(X_Test_2)\n",
    "\n",
    "print(\"Remove_Punctuation: \")\n",
    "X_Test_2 = Remove_Punctuation(X_Test_2)\n",
    "\n",
    "print(\"Tf_IDF: \")\n",
    "X_Test_2 = vect.transform(X_Test_2)\n",
    "\n",
    "allocutions = discours(data,0,X_Test_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################\n",
    "# Prédictions du premier classifieur\n",
    "#########################################\n",
    "y_pred_1 = clf_LOG.predict(X_Test_2)\n",
    "y_pred_1 =post_traitement_2(y_pred_1)\n",
    "y_pred_1=Convertir_Target_ToLetter(y_pred_1)\n",
    "\n",
    "Classifieur_1_tache2_test = open(\"corpus.tache2.pred1\",\"w\") \n",
    "for (t,lettre) in zip(Target_Test_2,y_pred_1):\n",
    "    t=t.strip(\">\")\n",
    "    Classifieur_1_tache2_test.write(t+\":\"+lettre+\">\\n\") \n",
    "Classifieur_1_tache2_test.close() \n",
    "\n",
    "#########################################\n",
    "# Prédictions du deuxième classifieur\n",
    "#########################################\n",
    "y_pred_2 = clf_SVC.predict(X_Test_2)\n",
    "y_pred_2 =post_traitement_2(y_pred_2)\n",
    "y_pred_2=Convertir_Target_ToLetter(y_pred_2)\n",
    "\n",
    "Classifieur_2_tache2_test = open(\"corpus.tache2.pred2\",\"w\") \n",
    "for (t,lettre) in zip(Target_Test_2,y_pred_2):\n",
    "    t=t.strip(\">\")\n",
    "    Classifieur_2_tache2_test.write(t+\":\"+lettre+\">\\n\") \n",
    "Classifieur_2_tache2_test.close() \n",
    "\n",
    "#########################################\n",
    "# Prédictions du troisième classifieur\n",
    "#########################################\n",
    "y_pred_3 = clf_P.predict(X_Test_2)\n",
    "y_pred_3 =post_traitement_2(y_pred_3)\n",
    "y_pred_3=Convertir_Target_ToLetter(y_pred_3)\n",
    "\n",
    "Classifieur_3_tache2_test = open(\"corpus.tache2.pred3\",\"w\") \n",
    "for (t,lettre) in zip(Target_Test_2,y_pred_3):\n",
    "    t=t.strip(\">\")\n",
    "    Classifieur_3_tache2_test.write(t+\":\"+lettre+\">\\n\") \n",
    "Classifieur_3_tache2_test.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
